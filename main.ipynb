{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep learning on handwritten digits\n",
    "\n",
    "The MNIST (mixed National Institute of Standards and Technology) dataset ([https://en.wikipedia.org/wiki/MNIST_database](https://en.wikipedia.org/wiki/MNIST_database)) is a classic data set in machine learning. To develop our intuitions about the problem, we start with a simple linear classifier and achieve an average accuracy of $80\\%$. We then proceed to build a state-of-the-art convolutional neural network (CNN) and achieve an accuracy of over $98\\%$.\n",
    "\n",
    "This notebook is available on [https://github.com/jcboyd/deep-learning-workshop](https://github.com/jcboyd/deep-learning-workshop).\n",
    "\n",
    "A Docker image for this project is available on Docker hub:\n",
    "\n",
    "> $ docker pull jcboyd/deep-learning-workshop/:[cpu|gpu]\n",
    "\n",
    "> $ nvidia-docker run -it -p 8888:8888 jcboyd/deep-learning-workshop/:[cpu|gpu]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Machine Learning\n",
    "\n",
    "* Machine learning involves algorithms that find patterns in data.\n",
    "\n",
    "\n",
    "* This amounts to a form of *inductive reasoning*: inferring general rules from examples, with the view of reapplying them to new examples. *Learning by example*\n",
    "\n",
    "\n",
    "\n",
    "* This symbols are labeled (yes/no) $\\implies$ *supervised* learning problem\n",
    "\n",
    "\n",
    "*Murphy, Kevin P. Machine learning: a probabilistic perspective. MIT press, 2012. (Figure 1.1)*\n",
    "\n",
    "![img/induction.png](img/induction.png)\n",
    "\n",
    "* The corresponding dataset would look something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "from pandas import read_csv\n",
    "read_csv(open('data/shapes.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.1 Classifiers\n",
    "\n",
    "* The above is an example of a *classification* problem. \n",
    "\n",
    "\n",
    "* Each *observation* $\\mathbf{x}_i$ is represented by a vector of $D$ dimensions or *features* and has label $y_i$ denoting its class (e.g. yes or no).\n",
    "\n",
    "\n",
    "* Thus, our dataset of $N$ observations is,\n",
    "\n",
    "$$\\mathcal{D} = \\{\\mathbf{x}_i, y_i\\}_{i=1}^N,$$\n",
    "$$\\mathbf{x}_i \\in \\mathbb{R}^D, y_i \\in \\{1, \\dots, C\\}$$\n",
    "\n",
    "\n",
    "* The model will attempt to divide the feature space such that the classes are as separate as possible, creating a *decision boundary*.\n",
    "\n",
    "![img/separation.png](img/separation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Exploration\n",
    "\n",
    "* The MNIST dataset ([https://en.wikipedia.org/wiki/MNIST_database](https://en.wikipedia.org/wiki/MNIST_database)) is a classic dataset in machine learning.\n",
    "\n",
    "\n",
    "* Derived from a dataset of handwritten characters \"crowdsourced\" from US high school students.\n",
    "\n",
    "\n",
    "* It consists of 60,000 labelled images of handwritten digits 0-9, and a test set of a further 10,000 images.\n",
    "\n",
    "\n",
    "* Notably used as a benchmark in the development of convolutional neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# mnist = mnist.read_data_sets('MNIST_data', reshape=False, one_hot=False)\n",
    "\n",
    "(Xtr, Ytr), (Xte, Yte) = mnist.load_data()\n",
    "\n",
    "Xtr, Xval, Ytr, Yval = train_test_split(Xtr, Ytr, test_size=5000)\n",
    "\n",
    "# Xtr = mnist.train.images\n",
    "# Ytr = mnist.train.labels\n",
    "\n",
    "# Xval = mnist.validation.images\n",
    "# Yval = mnist.validation.labels\n",
    "\n",
    "# Xte = mnist.test.images\n",
    "# Yte = mnist.test.labels\n",
    "\n",
    "print('Training data shape: ', Xtr.shape)\n",
    "print('Training labels shape: ', Ytr.shape)\n",
    "print('Validation data shape: ', Xval.shape)\n",
    "print('Validation labels shape: ', Yval.shape)\n",
    "print('Test data shape: ', Xte.shape)\n",
    "print('Test labels shape: ', Yte.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* We can visualise our data samples with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src import vis_utils\n",
    "\n",
    "# from IPython.display import set_matplotlib_formats\n",
    "# set_matplotlib_formats('svg')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "idx = np.random.randint(len(Xtr))\n",
    "vis_utils.plot_image(ax, Xtr[idx, :, :], Ytr[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* As far as our model will be concerned, the images in the dataset are just vectors of numbers.\n",
    "\n",
    "\n",
    "* From this perspective, there is no fundamental difference between the MNIST problem, and the symbols problem above.\n",
    "\n",
    "\n",
    "* The observations just live in 784-dimensional space (28 $\\times$ 28 pixels) rather than 2-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(Xtr[idx].reshape((1, 784))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "vis_utils.plot_array(fig, Xtr, Ytr, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.3 Data Preprocessing\n",
    "\n",
    "* For our linear models, we will \"flatten\" the data for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# First, vectorise image data\n",
    "Xtr_rows = np.reshape(Xtr, (Xtr.shape[0], -1)).copy()\n",
    "Xval_rows = np.reshape(Xval, (Xval.shape[0], -1)).copy()\n",
    "Xte_rows = np.reshape(Xte, (Xte.shape[0], -1)).copy()\n",
    "\n",
    "# As a sanity check, print out the shapes of the data\n",
    "print('Training data shape: ', Xtr_rows.shape)\n",
    "print('Validation data shape: ', Xval_rows.shape)\n",
    "print('Test data shape: ', Xte_rows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A typical procedure prior to training is to normalise the data.\n",
    "\n",
    "\n",
    "* Here we subtract the *mean image*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mean_image = np.mean(Xtr, axis=0).reshape(1, 784)\n",
    "\n",
    "Xtr_rows = Xtr_rows - mean_image\n",
    "Xval_rows = Xval_rows - mean_image\n",
    "Xte_rows = Xte_rows - mean_image\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "vis_utils.plot_image(ax, mean_image.reshape(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2 Linear Classification\n",
    "\n",
    "* First we will assume a linear *score function*, that is, a prediction that is a linear combination of inputs and model *weights*,\n",
    "\n",
    "$$f(\\mathbf{x} ; \\mathbf{w}) = \\mathbf{w}^T\\mathbf{x} = w_1x_1 + w_2x_2 + \\dots + w_Dx_D$$\n",
    "\n",
    "\n",
    "* For `MNIST`, $D = 784$, and we need a weight for every pixel in an image.\n",
    "\n",
    "\n",
    "* The choice of model weights will be *inferred* from the data in a procedure called *training*.\n",
    "\n",
    "\n",
    "*Stanford Computer Vision course--Convolutional Neural Networks for Visual Recognition [http://cs231n.stanford.edu/](http://cs231n.stanford.edu/) *\n",
    "\n",
    "<div style=\"text-align:center\"><img src =\"img/linear.png\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Model training\n",
    "\n",
    "* Training involves computing a mathematical function that differentiates between observations from different classes--classifier.\n",
    "\n",
    "\n",
    "* We first decide on a form for the function $f$ to take, then we optimise its parameters $\\mathbf{w}$ over the dataset and a loss function, $\\mathcal{L}$,\n",
    "\n",
    "$$\\mathbf{w}^* = \\min_{\\mathbf{w}} \\sum_{i=1}^N \\mathcal{L}(f(\\mathbf{x}_i ; \\mathbf{w}), y_i)$$\n",
    "\n",
    "\n",
    "* The loss function measures how close the classification $f(\\mathbf{x}_i ; \\mathbf{w})$ of observations $\\mathbf{x}_i$ is to the true value $y_i$.\n",
    "\n",
    "\n",
    "* Training consists of finding the weights that minimise the loss over the training set.\n",
    "\n",
    "* The most common procedure for optimising a convex differentiable function is known as *gradient descent*,\n",
    "\n",
    "$$\\mathbf{w}^{(k+1)} = \\mathbf{w}^{(k)} - \\alpha\\nabla\\mathcal{L}(\\mathbf{w}^{(k)})$$\n",
    "\n",
    "where $\\alpha$ is referred to as the step size or *learning rate*. Thus, each iteration is a descent step, and we converge iteratively to a global minimum.\n",
    "\n",
    "![img/gradientdescent.png](img/gradientdescent.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from src.linear_models import MultiSVM, SoftmaxRegression\n",
    "\n",
    "# Perform bias trick\n",
    "Xtr_rows = np.append(Xtr_rows, np.ones((Xtr_rows.shape[0], 1)), axis=1)\n",
    "Xval_rows = np.append(Xval_rows, np.ones((Xval_rows.shape[0], 1)), axis=1)\n",
    "Xte_rows = np.append(Xte_rows, np.ones((Xte_rows.shape[0], 1)), axis=1)\n",
    "\n",
    "reg = 5e4\n",
    "batch_size = 200\n",
    "max_iters = 1500\n",
    "learning_rate = 1e-7\n",
    "\n",
    "model = SoftmaxRegression(Xtr_rows, Ytr)\n",
    "model.train(reg, batch_size, learning_rate, max_iters, Xval_rows, Yval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "num_test = Yte.shape[0]\n",
    "predictions = [model.predict(Xte_rows[i]) for i in range(num_test)]\n",
    "print('Error: %.02f%%' % (100 * (1 - float(sum(Yte == np.array(predictions))) / num_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from src.vis_utils import plot_confusion_matrix\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "classes = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "\n",
    "confusion_matrix = np.zeros((num_classes, num_classes), np.int32)\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    confusion_matrix[Yte[i]][predictions[i]] += 1\n",
    "\n",
    "plot_confusion_matrix(ax, confusion_matrix, classes, fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's look at some of the model's mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "false = np.where(np.not_equal(Yte, predictions))[0]\n",
    "idx = np.random.choice(false)\n",
    "print('Prediction: %d\\nTrue class: %d' % (predictions[idx], Yte[idx]))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "vis_utils.plot_image(ax, Xte[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In pixel-space, the model had moderate success in separating the image clusters.\n",
    "\n",
    "![img/normals.png](img/normals.png)\n",
    "\n",
    "* The optimised weights are those generalising maximally over each of the class observations\n",
    "\n",
    "\n",
    "* We can take each of the weight vectors and plot them as an image to visualise the template they have learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4))\n",
    "vis_utils.plot_weights(fig, model.W[:-1,:], classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep Learning\n",
    "\n",
    "Deep learning is characterised by the modeling of a hierarchy of abstraction in the input data. In the following we focus on applications to images, but note deep learning has seen great success in various fields from natural language processing to speech synthesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1 Features\n",
    "\n",
    "* Features provide a *representation* of the objects we want to classify. Designing features is arguably the most difficult and most important aspect of machine learning.\n",
    "\n",
    "\n",
    "* In the above we were operating purely on pixel features. One way we might improve is with *feature engineering* (expert knowledge), *feature extraction* (conventional techniques), *feature selection*, or *dimensionality reduction*.\n",
    "\n",
    "\n",
    "* Another approach is to create non-linear transformations based on a *kernel function* (see kernel methods).\n",
    "\n",
    "\n",
    "* Yet another approach is to build the feature learning into the model itself. This is the essence of *representation* or *deep learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Artificial Neural Networks\n",
    "\n",
    "* Neural networks model hidden layers between input and ouptut, passing through non-linear activation functions.\n",
    "\n",
    "\n",
    "* Neural networks are particularly amenable to hierarchical learning, as hidden layers are easily stacked.\n",
    "\n",
    "\n",
    "* (Loosely) inspired by the interaction of neurons in the human brain.\n",
    "\n",
    "\n",
    "![img/layers.png](img/layers.png)\n",
    "\n",
    "\n",
    "\n",
    "* Multiclass logistic regression:\n",
    "\n",
    "$$\\mathbf{f}(\\mathbf{x}) = \\text{softmax}(\\mathbf{W}\\mathbf{x}  + \\mathbf{b})$$\n",
    "\n",
    "where $\\mathbf{W} \\in \\mathbb{R}^{K \\times D}$ are the weights and $\\mathbf{b} \\in \\mathbb{R}^{K \\times 1}$ (sometimes incorporated into the weights--bias trick) and $\\text{softmax}(x) = \\frac{\\exp(x)}{\\sum_{x'}\\exp(x')}$ generalises the sigma logistic function.\n",
    "\n",
    "* Neural network (one hidden layer):\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{h}(\\mathbf{x}) &= \\sigma(\\mathbf{W}^{(1)}\\mathbf{x}  + \\mathbf{b}^{(1)}) & \\text{(hidden layer)} \\notag \\\\\n",
    "\\mathbf{f}(\\mathbf{x}) &= \\text{softmax}(\\mathbf{W}^{(2)}\\mathbf{x}  + \\mathbf{b}^{(2)}) & \\text{(output layer)}\\notag\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "* Deeper network? Just keep stacking!\n",
    "\n",
    "$$\\mathbf{f}(\\mathbf{x}) = \\sigma(\\mathbf{W}^{(M)}\\sigma(\\mathbf{W}^{(M-1)}(\\cdots\\sigma(\\mathbf{W}^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)})\\cdots)  + \\mathbf{b}^{(M-1)})  + \\mathbf{b}^{(M)})$$\n",
    "\n",
    "\n",
    "* No longer a linear model, outputs are non-linear combinations of inputs and model parameters.\n",
    "\n",
    "\n",
    "* Non-convex, but still differentiable and trainable using gradient descent. Backpropagation algorithm computes the gradients by repeated application of the chain rule.\n",
    "\n",
    "\n",
    "* Pros (+): Greater flexibility (universal approximator), built-in feature extraction.\n",
    "\n",
    "\n",
    "* Cons (-): Harder to train (not convex), theory relatively underdeveloped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Convolutional Neural Networks\n",
    "\n",
    "Recall from signal processing, the *convolution* between two functions,\n",
    "\n",
    "$$(f * g)(t) \\triangleq \\int_{-\\infty}^{+\\infty}f(\\tau)g(t-\\tau)d\\tau$$\n",
    "\n",
    "In image processing, a convolution between an image $\\mathbf{I}$ and *kernel* $\\mathbf{K}$ of size $d \\times d$ and centered at a given pixel $(x, y)$ is defined as,\n",
    "\n",
    "$$(\\mathbf{I} * \\mathbf{K})(x, y) = \\sum_{i = 1}^{d}\\sum_{j = 1}^{d} \\mathbf{I}(x + i -d/2, y + j - d/2) \\times \\mathbf{K}(i, j)$$\n",
    "\n",
    "The dimension $d \\times d$ is referred to as the $\\textit{receptive field}$ of the convolution.\n",
    "\n",
    "![img/convolve.png](img/convolve.png)\n",
    "\n",
    "![img/convolution.jpg](img/convolution.jpg)\n",
    "\n",
    "* Convolutional Neural Networks (CNN) are a type of feed-forward neural network wired so as to perform convolutions (image processing) on input data.\n",
    "\n",
    "\n",
    "* Rather than one weight per pixel as before, the weights for a layer are restricted to a small, square kernel. This kernel is convolved with the local region at every pixel in the input image.\n",
    "\n",
    "\n",
    "* A convolutional layer therefore produces an *activation map*, a new image where regions responding to the kernel are \"activated\" (give a high score).\n",
    "\n",
    "\n",
    "* As such, feature extraction is built into the classifier and optimised w.r.t the same loss function (*representation learning*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 CNN architectures\n",
    "\n",
    "* Convolutional Neural Networks (CNNs) comprise of a series of layers called an architecture. This usually conists of some convolutional layers for feature extraction, followed by traditional fully connected layers for classification.\n",
    "\n",
    "\n",
    "* LeNet is the original network architecture of CNNs, introduced by Yann Lecun in the 90s.\n",
    "\n",
    "![img/lenet.png](img/lenet.png)\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{H}_1 &= \\sigma(\\mathbf{X} * \\mathbf{K}^{(1)}) & \\text{(first convolutional layer)}\\notag \\\\\n",
    "\\mathbf{P}_1 &= \\text{maxpool}(\\mathbf{H}_1) & \\text{(first pooling layer)}\\notag \\\\\n",
    "\\mathbf{H}_2 &= \\sigma(\\mathbf{P}_1 * \\mathbf{K}^{(2)}) & \\text{(second convolutional layer)} \\notag \\\\\n",
    "\\mathbf{P}_2 &= \\text{maxpool}(\\mathbf{H}_2) & \\text{(second pooling layer)} \\notag \\\\\n",
    "\\mathbf{F}_1 &= \\sigma(\\mathbf{W}^{(1)}\\mathbf{P}_2 + \\mathbf{b}^{(1)}) & \\text{(first fully-connected layer)} \\notag \\\\\n",
    "\\mathbf{F}_2 &= \\sigma(\\mathbf{W}^{(2)}\\mathbf{F}_1 + \\mathbf{b}^{(2)}) & \\text{(second fully-connected layer)} \\notag \\\\\n",
    "\\mathbf{f}(\\mathbf{X}) &= \\text{softmax}(\\mathbf{W}^{(3)}\\mathbf{F}_2 + \\mathbf{b}^{(3)}) & \\text{(output layer)} \\notag\n",
    "\\end{align}\n",
    "\n",
    "* $\\mathbf{X} \\in \\mathbb{R}^{32 \\times 32}$ are the input images and $\\mathbf{H}_1 \\in \\mathbb{R}^{6 \\times 28 \\times 28}$, $\\mathbf{H}_2 \\in \\mathbb{R}^{16 \\times 10 \\times 10}$ are (resp.) the 6 and 16 activation maps from the convolutional layers. The convolutional kernels are $\\mathbf{K}^{(1)} \\in \\mathbb{R}^{6\\times5\\times5}$ and $\\mathbf{K}^{(2)} \\in \\mathbb{R}^{16\\times5\\times5}$ i.e. 6 kernels of size $5\\times5$ kernels for the first convolutional layer, 16 kernels of size $5\\times5$ for the second. Multiple kernels are able to model different image motifs.\n",
    "\n",
    "\n",
    "* Note that the reduction in size after each convolution is due to convolutions not being performed at the borders (aka *valid* convolution). It is, however, more common to *pad* the input images with zeros to allow convolution on every pixel, thereby preserving the input size. In our model, we have $28 \\times 28$ inputs that will be zero-padded.\n",
    "\n",
    "\n",
    "* The maxpool function scales down (*downsamples*) the input by selecting the greatest activation (most intense pixel) in each (typically) $2 \\times 2$ block. Thus, each pooling layer halves the resolution.  $\\mathbf{P}_1 \\in \\mathbb{R}^{14 \\times 14}$, $\\mathbf{P}_2 \\in \\mathbb{R}^{5 \\times 5}$ This is instrumental in forming hierarchical layers of abstraction.\n",
    "\n",
    "\n",
    "* The first fully-connected layer, $\\mathbf{F}_1 \\in \\mathbb{R}^{120}$ concatenates the 16 activation maps of size $5\\times5$ vector. The rest of the network is like a traditional fully-connected network with $\\mathbf{F}_2 \\in \\mathbb{R}^{84}$ and a $10 \\times 1$ output layer.\n",
    "\n",
    "\n",
    "* Though far from convex, the score function remains differentiable (just sums and products of weights interspersed with activations), and can be trained with gradient descent + backpropagation.\n",
    "\n",
    "\n",
    "* A lot of deep learning research has been focused on improving learning: ReLU (more efficient activation function), Nestorov momentum/RMSprop/Adam (better optimisation), batch normalisation (weight stability), dropout (powerful regularisation).\n",
    "\n",
    "\n",
    "![img/lenet.png](img/alexnet.png)\n",
    "\n",
    "* CNNs were the breakout success in 2012 that won the ImageNet image classification challenge. *AlexNet* was a *deep* architecture that widened and deepened LeNet ($224 \\times 224$) input images.\n",
    "\n",
    "\n",
    "* The result was quickly followed by a paradigm shift in computer vision research, supplanting tailored feature extraction and reinstating neural networks in the state of the art.\n",
    "\n",
    "\n",
    "* CNNs have since surmounted long-standing challenges in artificial intelligence (e.g. AlphaGo).\n",
    "\n",
    "\n",
    "**References:**\n",
    "\n",
    "\n",
    "* *LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. \"Deep learning.\" Nature 521.7553 (2015): 436-444.*\n",
    "\n",
    "\n",
    "* *LeCun, Yann, et al. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE 86.11 (1998): 2278-2324.*\n",
    "\n",
    "\n",
    "* *Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet classification with deep convolutional neural networks.\" Advances in neural information processing systems. 2012.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Tensor Flow\n",
    "\n",
    "* Python machine learning framework developed by Google Brain (deep learning group). Competes with $\\texttt{Caffe}$, $\\texttt{Theano}$, and $\\texttt{Torch}$. Higher-level APIs exist such as $\\texttt{Keras}$ and $\\texttt{Lasagne}$.\n",
    "\n",
    "\n",
    "* Originally proprietary, made open source (Apache 2.0) in late 2015\n",
    "\n",
    "\n",
    "* Model is built into a *computational graph*\n",
    "\n",
    "\n",
    "* *Tensors* (multi-dimensional arrays of data) are passed through a *dataflow* computational graph (stream processing paradigm).\n",
    "\n",
    "\n",
    "* *CNN code adapted from demo code in the official TensorFlow Docker image*\n",
    "\n",
    "![img/tensorflowlogo.png](img/tensorflowlogo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialise our model. In TensorFlow, this consists of declaring the operations (and relationships thereof) required to compute the forward pass (from input to loss function) of the model (see `src/cnn.py`). Note that this is done in a declarative fashion, and it may be counter-intuitive that this code is only run once to initialise the computational graph. Actual forward passes are performed via a `tf.Session()` variable, with mini-batches passed through the graph to a nominal reference node (for example, the loss node). TensorFlow then knows how to backpropagate through each graph operation. This paradigm has its drawbacks, however, as it is highly verbose, and error traces are often opaque. `PyTorch`, a TensorFlow alternative, addresses this problem by keeping everything interpreted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten,  Activation, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense\n",
    "\n",
    "\n",
    "def LeNet(input_shape, nb_classes):\n",
    "\n",
    "    input_img = Input(shape=input_shape, name='input')\n",
    "\n",
    "    x = Conv2D(16, kernel_size=(3, 3), padding='same')(input_img)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = Conv2D(32, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = Activation('relu', name='cnn_code')(x)\n",
    "\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(nb_classes)(x)\n",
    "    x = Activation('softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_img, outputs=x)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = LeNet(input_shape=(28, 28, 1), nb_classes=10)\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.6 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "x_train = Xtr[..., np.newaxis]\n",
    "y_train = to_categorical(Ytr)\n",
    "\n",
    "x_val = Xval[..., np.newaxis]\n",
    "y_val = to_categorical(Yval)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('./weights_lenet.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   save_best_only=True)\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          validation_data=(x_val, y_val),\n",
    "          epochs=10,\n",
    "          callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./weights_lenet.h5')\n",
    "\n",
    "x_test = Xte[..., np.newaxis]\n",
    "\n",
    "pred = np.argmax(model.predict(x_test), axis=1).astype(np.int8)\n",
    "correct = np.sum(pred == Yte)\n",
    "\n",
    "print('Test error: %.02f%%' % (100 * (1 - float(correct) / float(pred.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "confusion_matrix = np.zeros((num_classes, num_classes), np.int32)\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    confusion_matrix[Yte[i]][pred[i]] += 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_confusion_matrix(ax, confusion_matrix, classes, fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Current world record: 0.21% error from *ensemble* of 5 CNNs with data augmentation\n",
    "\n",
    "*Romanuke, Vadim. \"Parallel Computing Center (Khmelnitskiy, Ukraine) represents an ensemble of 5 convolutional neural networks which performs on MNIST at 0.21 percent error rate.\". Retrieved 24 November 2016.\"*\n",
    "\n",
    "* We can plot the activation maps of the following image as it passes through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(x_test.shape[0])\n",
    "\n",
    "img = x_test[idx:idx+1]\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "vis_utils.plot_image(ax, img[0, ..., 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, the 32 activations of the first convolutional layer ($28 \\times 28$ px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = Model(inputs=model.input, outputs=model.layers[3].output)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "vis_utils.plot_activation_maps(fig, conv1.predict(img), 2, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the 64 activations of the second convolutional layer ($14 \\times 14$ px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = Model(inputs=model.input, outputs=model.layers[6].output)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "vis_utils.plot_activation_maps(fig, conv2.predict(img), 4, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For a live demo of activations in deep networks, see the Deep Visualisation Toolbox (https://www.youtube.com/watch?v=AgkfIQ4IGaM)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
